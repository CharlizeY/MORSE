---
title: "ST340 Assignment 2"
author: ' u1727406 '
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Q1 Expectation Maximization

#### (a)
Consider the following:
\[
\begin{align}
f(\mu_{1:K})
&=\sum_{i=1}^{n}\sum_{k=1}^{K}\gamma_{ik}\log
p(x_{i}|\mu_{k})\\
&=\sum_{i=1}^{n}\sum_{k=1}^{K}\gamma_{ik}\log(\prod_{j=1}^{p}\mu_{kj}^{x_{ij}}(1-\mu_{kj})^{1-x_{ij}})\\
&=\sum_{i=1}^{n}\sum_{k=1}^{K}\gamma_{ik}\sum_{j=1}^{p}\log(\mu_{kj}^{x_{ij}}(1-\mu_{kj})^{1-x_{ij}})\\
&=\sum_{i=1}^{n}\sum_{k=1}^{K}\gamma_{ik}\sum_{j=1}^{p}\log(\mu_{kj}^{x_{ij}})+\log((1-\mu_{kj})^{1-x_{ij}})\\
&=\sum_{i=1}^{n}\sum_{k=1}^{K}\gamma_{ik}\sum_{j=1}^{p}(x_{ij}log(\mu_{kj})+(1-x_{ij})log(1-\mu_{kj}))\\
\end{align}
\]

<br>
Compute the partial derivatives and set them to 0 to get the unique stationary point:
\[
\begin{align}
    \frac{\partial f}{\partial\mu_{kj}}
    =\sum_{i=1}^{n}\gamma_{ik}(\frac{x_{ij}
    }{\mu_{kj}}-\frac{1-x_{ij}}{1-\mu_{kj}})=0
    &\iff\sum_{i=1}^{n}\gamma_{ik}\frac{x_{ij}
    }{\mu_{kj}}=\sum_{i=1}^{n}\gamma_{ik}(\frac{1-x_{ij}
    }{1-\mu_{kj}})\\
    &\iff(1-\mu_{kj})\sum_{i=1}^{n}\gamma_{ik}x_{ij}
    =\mu_{kj}\sum_{i=1}^{n}\gamma_{ik}(1-x_{ij})\\
    &\iff\sum_{i=1}^{n}\gamma_{ik}x_{ij}
    =\mu_{kj}\sum_{i=1}^{n}\gamma_{ik}\\
    &\iff\mu_{kj}=\frac{\sum_{i=1}^{n}\gamma_{ik}x_{ij}}{\sum_{i=1}^{n}\gamma_{ik}}\\
    &\iff\mu_{k}=\frac{\sum_{i=1}^{n}\gamma_{ik}x_{i}}{\sum_{i=1}^{n}\gamma_{ik}}\\
\end{align}    
\]

<br><br>

#### (b)(i)
```{r,eval=FALSE}
load("/Users/Cherry0904/Desktop/Third Year/ST340/20newsgroups.rdata")
n<-16242; p<-100
K.actual <- 4
xs <- documents
count<-rep(0,p)
for(i in 1:p){
count[i]<-sum(documents[,i])
}
mus.actual<-matrix(0,K.actual,p)
for(i in 1:p){
wordvector<-newsgroups*documents[,i]
for(k in 1:K.actual){
mus.actual[k,i]<-sum(wordvector==k)/count[i]
 }
}
```

Run the algorithm (the middle 283 iterative results are omitted for simplicity):
```{r,eval=FALSE}
source("em_mixture_bernoullis.R")
print(system.time(out <- em_mix_bernoulli(xs,K.actual)))
```

Measure the accuracy-how the learned parameters are close to the truth:
```{r,eval=FALSE}
v<-matrix(0,K.actual,K.actual)
m<-rep(0,4)
for (i in 1:K.actual){
for (k in 1:K.actual){
v[i,k] <- sum(abs(out$mus[k,]-mus.actual[i,]))
# return the index of minimum entry for each row
m[i]<-which.min(v[i,])
 }
}
print(v)
print(m)
# Although m returns 3,1,3,4, it's easy to spot from matrix v that the minimum total sum of difference occurs when out$mus has the order of rows: 3,1,2,4.

# calculate ws, the initial cluster assignment probablity
lws.new<-out$lws
lws.new[1]=out$lws[3]
lws.new[2]=out$lws[1]
lws.new[3]=out$lws[2]
lws.new[4]=out$lws[4]
exp(lws.new)

# calculate mu, the mixture parameter
mus.new=out$mus
mus.new[1,]=out$mus[3,]
mus.new[2,]=out$mus[1,]
mus.new[3,]=out$mus[2,]
mus.new[4,]=out$mus[4,]

vm <- sum(abs(mus.new-mus.actual))/p/4
print(vm)
if (vm > .3) print("probably not working") else print("might be working")
```
Comments: 
<br>

According to the calculated ws (the initial probability that data belonging to each cluster), the "sci" topic is most popular, with more than 1/2 (0.525) probability that a post belongs to this topic. This is followed the "comp" topic, which gives probability above 1/4 (0.269); the least popular topic is "talk", with probability less than 0.10. 

<br>
By measuring the accuracy in the codes above, I get the mean absolute difference between the calculated mu and the true mu is approximately 0.201 (which is smaller than 0.3), so we can conclude that the clustering provided by the algorithm is reasonably accurate.


<br><br><br>

## Q2 Two-armed Bernoulli bandits

#### (a) 
Firstly, implement the epsilon-decreasing strategy:
```{r,eval=FALSE}
library(mvtnorm)
# Set Bernoulli success parameters for each arm
ps <- c(0.6,0.4)
# Set the epsilon-n sequence in the form of Cn^(-k)
epsilon_seq<-function(C,k){
    epsilon<-rep(0,n)
for (i in 1:n) {
    epsilon[i]<-min(1,C/(i^k))
}
    return(epsilon)
}

# epsilon-decreasing algorithm, runs for n steps:
epsilon.decreasing <- function(ps,epsilon,n) {
  as <- rep(0,n)
  rs <- rep(0,n)
  ns <- rep(0,2); ss <- rep(0,2)
  
  # at first, play each arm once
  for (i in 1:2) {
    a <- i
    r <- runif(1) < ps[a]
    ns[a] <- ns[a] + 1
    ss[a] <- ss[a] + r
    as[i] <- a
    rs[i] <- r
  }
  
  ## then follow the epsilon decreasing strategy
  for (i in 3:n) {
    if (runif(1) < epsilon[i]) {
      a <- sample(2,1)
    } else { 
      a <- which.max(ss/ns)
    }
    r <- runif(1) < ps[a]
    
    ns[a] <- ns[a] + 1
    ss[a] <- ss[a] + r
    
    as[i] <- a
    rs[i] <- r
  }
  return(list(as=as,rs=rs))
}
```
Secondly, implement the following code for Thompson Sampling:
```{r,eval=FALSE}
sample_arm.bernoulli  <- function(ns,ss) {
alphas <- 1 + ss # successes
betas <- 1 + ns - ss # failures
t1 <- rbeta(1,alphas[1],betas[1])
t2 <- rbeta(1,alphas[2],betas[2])
if (t1 > t2) {
return(1)
} else {
return(2)
 }
}

thompson.bernoulli <- function(ps,n) {
  as <- rep(0,n)
  rs <- rep(0,n)
  
  ## number of times each arm has been played
  ## and number of corresponding successes
  ns <- rep(0,2); ss <- rep(0,2)
  
  for (i in 1:n) {
    a <- sample_arm.bernoulli(ns,ss)
    r <- runif(1) < ps[a]
    ns[a] <- ns[a] + 1
    ss[a] <- ss[a] + r
    as[i] <- a
    rs[i] <- r
  }
  return(list(as=as,rs=rs))
}
```


<br><br>

#### (b)
As n increases, the sequence $\epsilon_{n}=\min\{1,Cn^{-1}\}$ decreases,so we would be less likely to play an arm randomly with probalibility  $\epsilon_{n}$, instead, it would be more likely for us to play the arm with the best success rate so far, and playing more times of the "success arm" means our average rewards should approach 0.6 at an increasing speed.

Check:
Run the algorithms and see the performance of `epsilon.decreasing` when $\epsilon_{n}=\min\{1,Cn^{-1}\}$ and plot the average award over time:
```{r,eval=FALSE}
# here choose C in epsilon_seq to be 5
epsilon<-epsilon_seq(5,1)
ed.out.1 <- epsilon.decreasing(ps=ps,epsilon=epsilon,n=1e4)
sum(ed.out.1$rs)/length(ed.out.1$rs)
```
```{r,eval=FALSE}
average_rs.1<-rep(0,n)
for(i in 1:n){
  average_rs.1[i]<-sum(ed.out.1$rs[1:i])/length(ed.out.1$rs[1:i])
}
plot(1:n,average_rs.1,type="l",col="red",ylab="average reward",xlab="n")
abline(0.6,0,col="black")
```
As seen in the graph, the average reward approaches 0.6 at an increasing speed, which is consistent with the description.


<br><br>

#### (c)
As n increases, the sequence $\epsilon_{n}=\min\{1,Cn^{-2}\}$ decreases even faster than the sequence in (b), which means each time we would play the "success" arm with even higher probability (and potentially start to play the "success" arm sooner), and our average reward should approach to 0.6 even faster than (b).

Check:
Run the algorithms and see the performance of `epsilon.decreasing` when $\epsilon_{n}=\min\{1,Cn^{-2}\}$ and plot the average award over time:
```{r,eval=FALSE}
epsilon<-epsilon_seq(5,2)
ed.out.2 <- epsilon.decreasing(ps=ps,epsilon=epsilon,n=1e4)
sum(ed.out.2$rs)/length(ed.out.2$rs)
```

```{r,eval=FALSE}
average_rs.2<-rep(0,n)
for(i in 1:n){
  average_rs.2[i]<-sum(ed.out.2$rs[1:i])/length(ed.out.2$rs[1:i])
}
plot(1:n,average_rs.2,type="l",col="blue",ylab="average reward",xlab="n",ylim=range(c(0.2,0.8)))
abline(0.6,0,col="black")
par(new=TRUE)
plot(1:n,average_rs.1,type="l",col="red",ylab="average reward",xlab="n",ylim=range(c(0.2,0.8)))
legend("bottomright",legend=c("Epsilon in (b)", "Epsilon in (c)"),
       col=c("red", "blue"),lty=1,cex=0.8)
```
As seen in the graph, the average reward approaches 0.6 at a faster speed than (b) (the blue line converges to 0.6 faster than the red line), which is consistent with the description.

<br><br>

#### (d)
Compare and Contrast:
compare the average reward for ED and Thompson:
```{r,eval=FALSE}
sum(ed.out.1$rs)/length(ed.out.1$rs)
```
```{r,eval=FALSE}
sum(ed.out.2$rs)/length(ed.out.2$rs)
```
```{r,eval=FALSE}
thompson.bernoulli.out <- thompson.bernoulli(ps=ps,n=1e4)
sum(thompson.bernoulli.out$rs)/length(thompson.bernoulli.out$rs)
```
In this single run of algorithms, the Epsilon-decreasing algorithm in (b) gives the average award that is the closest to 0.6 (0.5983), followed by the Epsilon-decreasing algorithm in (c), which gives an average reward 0.5978. The  Thompson's sampling method gives the average award the most deviated from 0.6, which is 0.5974.


<br>

Plot the average awards over time for ED and Thompson:
```{r,eval=FALSE}
average_rs.T<-rep(0,n)
for(i in 1:n){
  average_rs.T[i]<-sum(thompson.bernoulli.out$rs[1:i])/length(thompson.bernoulli.out$rs[1:i])
}

# plot for ED in (c)
plot(1:n,average_rs.2,type="l",col="blue",ylab="average reward",xlab="n",ylim=range(c(0,0.8)))
abline(0.6,0,col="black")

# plot for ED in (b)
par(new=TRUE)
plot(1:n,average_rs.1,type="l",col="red",ylab="average reward",xlab="n",ylim=range(c(0,0.8)))

# plot for Thompson
par(new=TRUE)
plot(1:n,average_rs.T,type="l",col="green",ylab="average reward",xlab="n",ylim=range(c(0,0.8)))

legend("bottomright",legend=c("Epsilon in (b)", "Epsilon in (c)","Thompson"),lty=1,col=c("red", "blue","green"),cex=0.8)
```
As seen in the graph, in this single run of algorithms, the Thompson's method initially converges faster to 0.6 than ED algorithms. However, the ED algorithms starts to converge faster than Thompson from later on, after around n=1000. 



<br>

Compare the lost reward/regret:
```{r,eval=FALSE}
sum(0.6-ed.out.1$rs)
sum(0.6-ed.out.2$rs)
sum(0.6-thompson.bernoulli.out$rs)
```
ED in (b) has the least lost reward 17, followed by the ED in (c), which has the lost reward 22.Thompson's Sampling has the largest lost reward which is 26.


<br>

Plot the lost reward/regret over time for ED and Thompson:
```{r,eval=FALSE}
LR.1<-rep(0,n)
for(i in 1:n){
  LR.1[i]<-sum(0.6-ed.out.1$rs[1:i])
}

LR.2<-rep(0,n)
for(i in 1:n){
  LR.2[i]<-sum(0.6-ed.out.2$rs[1:i])
}

LR.T<-rep(0,n)
for(i in 1:n){
  LR.T[i]<-sum(0.6-thompson.bernoulli.out$rs[1:i])
}

# plot for ED in (b)
plot(1:n,LR.1,type="l",col="red",ylab="regret",xlab="n",ylim=range(c(-100,100)),xlim=range(c(0,10000)))

# plot for ED in (c)
par(new=TRUE)
plot(1:n,LR.2,type="l",col="blue",ylab="regret",xlab="n",ylim=range(c(-100,100)),xlim=range(c(0,10000)))

# plot for Thompson
par(new=TRUE)
plot(1:n,LR.T,type="l",col="green",ylab="regret",xlab="n",ylim=range(c(-100,100)),xlim=range(c(0,10000)))

legend("topleft",legend=c("Epsilon in (b)", "Epsilon in (c)","Thompson"),lty=1,col=c("red", "blue","green"),cex=0.6)
```
Overall, three methods all demonstrate a fluctuated pattern of regrets. Thompson's Sampling tends to have a relatively higher positive regret compared with both ED algorithms over time, while ED in (c) has the largest negative regret and fluctuates the most over time. ED in (b) seems to have the most stable regret and fluctuates the least compared with the other two.


<br><br><br>

## Q3 k-nearest neighbours
#### (a)        
```{r eval=FALSE}
knn.regression.test <- function(k,train.X,train.Y,test.X,test.Y,distances) {
  #define the distance matrix
  D<-matrix(0,nrow=nrow(train.X),ncol=nrow(test.X))
  estimates<-rep(0,length(test.X))
  D<-distances(train.X,test.X)
  # define Y as a matrix with the same dimensions as D
  Y<-matrix(train.Y,nrow=nrow(train.X),ncol=nrow(test.X))
  # sort each column in D independently
  D_sorted<-apply(D, 2, sort) 
  # sort columns of Y the same way as sorting columns in D
  Y_sorted<-matrix(Y[order(col(D), D)], byrow = FALSE, ncol = ncol(Y))
  D_sorted_k<-D_sorted[1:k,]
  Y_sorted_k<-Y_sorted[1:k,]
  estimates<-colSums(Y_sorted_k*1/D_sorted_k)/colSums(1/D_sorted_k)
  print(sum((test.Y-estimates)^2))
}
```
<br><br>

#### (b) 
**Toy dataset 1**:
```{r eval=FALSE}
distances.l1 <- function(x,y) {
  apply(y,1,function(p) apply(x,1,function(q) sum(abs(p-q))))
}
n <- 100
train.X <- matrix(sort(rnorm(n)),n,1)

train.Y <- (train.X < -0.5) + train.X*(train.X>0)+rnorm(n,sd=0.03)
plot(train.X,train.Y)
test.X <- matrix(sort(rnorm(n)),n,1)
test.Y <- (test.X < -0.5) + test.X*(test.X>0)+rnorm(n,sd=0.03)

k <- 2
knn.regression.test(k,train.X,train.Y,test.X,test.Y,distances.l1)
# this gives value 1.824307

# plot the error with respect to k
result<-rep(0,nrow(test.X)-1)
for(j in 2:nrow(test.X)){
  result[j-1]<-knn.regression.test(j,train.X,train.Y,test.X,test.Y,distances.l1)
}
plot(2:nrow(test.X),result,xlab="k",ylab="residual sum of square")

# as you can see in the graph, in this single run of algorithm, the optimal k is 20 (which minimises the residual sum of square error.) The error tends to increase with the value of k, which makes sense as using y values of further points would likely to make the predictions worse.
```

**Toy dataset 2**:
```{r,eval=FALSE}
train.X <- matrix(rnorm(200),100,2) 
train.Y <- train.X[,1]
test.X <- matrix(rnorm(100),50,2) 
test.Y <- test.X[,1]

k <- 3 
knn.regression.test(k,train.X,train.Y,test.X,test.Y,distances.l1)
# this gives value 2.306493

# plot the error with respect to k
result<-rep(0,nrow(test.X)-1)
for(j in 2:nrow(test.X)){
  result[j-1]<-knn.regression.test(j,train.X,train.Y,test.X,test.Y,distances.l1)
}
plot(2:nrow(test.X),result,xlab="k",ylab="residual sum of square")

# as you can see in the graph, in this single run of algorithm, the optimal k is 5 (which minimises the residual sum of square error, note that k starts from 2.) The error tends to increase with the value of k, which makes sense as using y values of further points would likely to make the predictions worse.
```
<br><br>

#### (c)
```{r,eval=FALSE}
distances.l2 <- function(x,y) {
  apply(y,1,function(p) apply(x,1,function(q) sqrt(sum((p-q)^2))))
}

library("lasso2")
data(Iowa) 
train.X=as.matrix(Iowa[seq(1,33,2),1:9]) 
train.Y=c(Iowa[seq(1,33,2),10]) 
test.X=as.matrix(Iowa[seq(2,32,2),1:9]) 
test.Y=c(Iowa[seq(2,32,2),10])

knn.regression.test <- function(k,train.X,train.Y,test.X,test.Y,distances) {
  D<-matrix(0,nrow=nrow(train.X),ncol=nrow(test.X))
  estimates<-rep(0,length(test.X))
  D<-distances(train.X,test.X)
  Y<-matrix(train.Y,nrow=nrow(train.X),ncol=nrow(test.X))
  D_sorted<-apply(D, 2, sort) 
  Y_sorted<-matrix(Y[order(col(D), D)], byrow = FALSE, nrow = nrow(Y))
  D_sorted_k<-D_sorted[1:k,]
  Y_sorted_k<-Y_sorted[1:k,]
  estimates<-colSums(Y_sorted_k*1/D_sorted_k)/colSums(1/D_sorted_k)
  # print the estimates of yields for year 1931,1933...
  print(cbind(test.X[,1],estimates))
  print(sum((test.Y-estimates)^2))
}

k <- 5
knn.regression.test(k,train.X,train.Y,test.X,test.Y,distances.l2)
# this gives value 1751.707

# plot the error with respect to k
result<-rep(0,nrow(test.X)-1)
for(j in 2:nrow(test.X)){
  result[j-1]<-knn.regression.test(j,train.X,train.Y,test.X,test.Y,distances.l1)
}
plot(2:nrow(test.X),result,xlab="k",ylab="residual sum of square")

# as you can see in the graph, in this single run of algorithm, the optimal k is 4 (which minimises the residual sum of square error.) The error flunctuates with the value of k, and there is no clear pattern demonstrated.
```

<br><br>

#### (d)
```{r,eval=FALSE}
train.X.new<-as.data.frame(train.X)
test.X.new<-as.data.frame(test.X)

#Ordinary Least Squares
OLS<-lm(train.Y~.,train.X.new)
Y_OLS<-predict(OLS,newdata=test.X.new)
sum((test.Y-Y_OLS)^2)

#Ridge regression, here choose K (the ridge biasing parameter) to be 5
library(lmridge)
ridge<-lmridge(train.Y~.,train.X.new,K=5)
Y_ridge<-predict.lmridge(ridge,newdata=test.X.new)
sum((test.Y-Y_ridge)^2)
```
Comparison: 
<br>
The least residual standard error of k-means neighbours is around 1550 (when k=4), which is smaller than that of OLS and ridge regression, which are 1891.556 and 1743.908, respectively. This may suggest that k-means neighbours regression gives better predictions in this example compared with the other two regression methods.
